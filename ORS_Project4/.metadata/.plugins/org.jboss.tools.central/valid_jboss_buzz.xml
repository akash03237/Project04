<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to install VMs and Ansible Automation Platform on Mac M1</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/25/how-install-vms-and-ansible-automation-platform-mac-m1" /><author><name>Arnav Bhati</name></author><id>53808b9c-70a3-46fe-b29a-3b371aa6152c</id><updated>2022-10-25T07:00:00Z</updated><published>2022-10-25T07:00:00Z</published><summary type="html">&lt;p&gt;Most of the time, we create our own Ansible playground to test Playbooks, features of the Red Hat Ansible Automation Platform (AAP), and many other things. We create our dedicated environment to test AAP for faster troubleshooting and resolution of customer issues.&lt;/p&gt; &lt;p&gt;Previously, it was a pain to install virtual machines on a Mac with an M1 chip. But now, &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) VMs can be easily installed using &lt;a href="https://mac.getutm.app/"&gt;UTM&lt;/a&gt;, a system emulator and virtual machine host for macOS.&lt;/p&gt; &lt;h2&gt;How to install a RHEL VM using UTM&lt;/h2&gt; &lt;p&gt;It should take less than two hours to finish the setup, including installation.&lt;/p&gt; &lt;p&gt;To create VMs (RHEL 8.6 for automation controller) on Mac M1 machines using UTM, follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Download and install &lt;a href="https://mac.getutm.app/"&gt;UTM&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;After installation, click “+” to create a VM.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;virtualize&lt;/strong&gt; and click &lt;strong&gt;Linux&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Download the &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;RHEL 8&lt;/a&gt; ISO DVD image.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click on &lt;strong&gt;browse &lt;/strong&gt;and select the image. Then click &lt;strong&gt;continue&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the RAM for the automation platform as per the AAP-2.2 minimum requirements &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.2/html/red_hat_ansible_automation_platform_installation_guide/planning-installation#automation_controller"&gt;document&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the 100GB disk size, then click &lt;strong&gt;continue&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Give the appropriate name to your VM, then check the &lt;strong&gt;Open VM Settings&lt;/strong&gt; button.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Navigate to the &lt;strong&gt;network&lt;/strong&gt; section. Under &lt;strong&gt;network mode,&lt;/strong&gt; select &lt;strong&gt;bridged (advanced)&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Play&lt;/strong&gt; button to start the installation.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Fill in the details you need to install RHEL 8 as your VM.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;After the installation is complete, close the VM.&lt;/li&gt; &lt;li&gt; &lt;p&gt;Click the VM name in UTM and unselect the CD/DVD option. Now your RHEL machine will start normally.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Enable the wired connection from the network connection settings.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 1 illustrates the installed RHEL 8 VM.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/VM-screenshot.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/VM-screenshot.jpg?itok=Knzu8wsp" width="600" height="371" alt="A screenshot of the installed RHEL 8 virtual machine." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Your VM is ready to use! &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Install AAP on RHEL VM&lt;/h2&gt; &lt;p&gt;To get your VM ready to install AAP 2.2, follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Open the terminal and type &lt;code&gt;ifconfig&lt;/code&gt;. Note that the IP address will be in a similar subnet of your Mac IP address.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open a terminal in your Mac machine and connect from your Mac to the newly created VM via SSH.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Now your VM is ready to install AAP 2.2. Create a Red Hat Registry Service Account, following the instructions in the &lt;a href="https://access.redhat.com/RegistryAuthentication#creating-registry-service-accounts-6"&gt;Creating Registry Service Accounts guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now install AAP 2.2 by following these steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Download AAP-2.2 from the &lt;a href="https://access.redhat.com/downloads/content/480/ver=2.1/rhel---8/2.1/x86_64/product-software" id="isPasted"&gt;Red Hat Ansible Automation Platform Product Software&lt;/a&gt; site.&lt;/li&gt; &lt;li&gt;Edit the inventory file and add the correct hostname for your automation controller.&lt;/li&gt; &lt;li&gt;Mandatory parameters for inventory file: &lt;ul&gt;&lt;li&gt;[automationcontroller]&lt;/li&gt; &lt;li&gt;[all:vars]: Admin password for your AAP controller&lt;/li&gt; &lt;li&gt;pg_password: Add password for database.&lt;/li&gt; &lt;li&gt;registry_username and registry_password: Credentials for container registry to pull execution environment images.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Run the &lt;strong&gt;setup.sh&lt;/strong&gt; script.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The following listing illustrates a snippet of the inventory file for AAP-2.2 installation.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# control.example node_type=control # hybrid.example node_type=hybrid # hybrid2.example &amp;lt;- this will default to hybrid [automationcontroller] #192.168.0.9 ansible_connection=local #127.0.0.1 ansible_connection=local aap ansible_connection=local [automationcontroller:vars] peers=execution_nodes&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After successfully finishing the setup script, you can access the automation controller server from &lt;code&gt;https://&lt;TOWER_SERVER_NAME&gt;/&lt;/code&gt;. In this scenario we are using &lt;code&gt;aap&lt;/code&gt; as the hostname, so the URL will be &lt;code&gt;https://app/&lt;/code&gt; to access our automation controller UI.&lt;/p&gt; &lt;p&gt;Figure 2 illustrates the UI for the AAP-2.2 controller.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/AAP-2.2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/AAP-2.2.png?itok=2w1r6ZMj" width="600" height="392" alt="Figure 3 : AAP controller UI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: AAP-2.2 controller UI&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Note that the newly created VM should define a hostname in &lt;code&gt;/etc/hostname&lt;/code&gt;. It would be best if you used this name in the automation controller. Otherwise, the installation will fail. Here's how you'd check the hostname of the controller machine in this file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[root@aap aap-2.2-test]# cat /etc/hostname aap&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Now that we have shown you how easy it is to install RHEL VMs using UTM, try it for yourself. Refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.2"&gt;Red Hat Ansible Automation Platform 2.2 Documentation&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.2/html/red_hat_ansible_automation_platform_installation_guide" id="isPasted"&gt;Red Hat Ansible Automation Platform Installation Guide&lt;/a&gt; for further information and resources. Feel free to comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/25/how-install-vms-and-ansible-automation-platform-mac-m1" title="How to install VMs and Ansible Automation Platform on Mac M1"&gt;How to install VMs and Ansible Automation Platform on Mac M1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Arnav Bhati</dc:creator><dc:date>2022-10-25T07:00:00Z</dc:date></entry><entry><title type="html">Eclipse Vert.x 3.9.14 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-3-9-14" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-3-9-14</id><updated>2022-10-25T00:00:00Z</updated><content type="html">Eclipse Vert.x version 3.9.14 has just been released.</content><dc:creator>Julien Viet</dc:creator></entry><entry><title>Podman expands to the Desktop</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/24/podman-expands-desktop" /><author><name>Stevan Le Meur, Tim deBoer</name></author><id>9c5e0744-8573-42ae-a2f9-bddc73643f88</id><updated>2022-10-24T20:59:37Z</updated><published>2022-10-24T20:59:37Z</published><summary type="html">&lt;p&gt;At KubeCon North America 2022, the Podman community unveiled introduced &lt;a href="https://podman-desktop.io/"&gt;Podman Desktop&lt;/a&gt;, a new tool for developers. Podman Desktop allows developers to install, configure, and keep their container engine (Podman) up to date with a GUI. This convenient GUI enables users to interact with containers and pods running in Podman (Figure 1).&lt;/p&gt; &lt;p&gt;Podman Desktop also provides an easy deployment to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; for developers working with &lt;a href="https://developers.redhat.com/topics/containers/"&gt;containers&lt;/a&gt; on their laptops, allowing a seamless transition from containers to pods and pods to Kubernetes. Podman expands to the desktop to provide a complete experience enabling developers to easily work with containers.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/PodmanDesktop-image.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/PodmanDesktop-image.jpg?itok=mxUPiiv5" width="600" height="451" alt="Screenshot of the Podman desktop." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Podman Desktop is available on Windows, Mac, and Linux. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Motivation for the Podman Desktop initiative&lt;/h2&gt; &lt;p&gt;Containers have become a fundamental part of a developer’s workflow. There is no avoiding them.&lt;/p&gt; &lt;p&gt;Containers are commonly used for:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Standing up a local database&lt;/li&gt; &lt;li&gt;Testing containers or a set of services via compose&lt;/li&gt; &lt;li&gt;Building an application to deploy on the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; container platform or some other Kubernetes variant&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;How do we interact with containers on the desktop today? The majority of developers use Docker Desktop for local containers. But most container engines do not have a UI. Instead, developers have been using terminal windows, IDE extensions, or Kubernetes consoles. Developers have also been learning &lt;code&gt;kubectl&lt;/code&gt; to interact with containers.&lt;/p&gt; &lt;p&gt;These tools have their place in spite of their inconsistencies and the difficulty of bridging or interacting with multiple environments. Over time, however, developers must deal with higher complexity and additional overhead. The gaps and discrepancies between developing on the desktop and target Kubernetes production environments are becoming more challenging. As a result, the Podman Desktop initiative started with the goal to minimize the discrepancies.&lt;/p&gt; &lt;h2&gt;An introduction to Podman Desktop&lt;/h2&gt; &lt;p&gt;The Podman community has been working on Podman Desktop for the past few months. The initial scope is to provide a single desktop GUI to help interact with Podman or other Kubernetes environments for application developers.&lt;/p&gt; &lt;p&gt;You can install Podman with one click and get containers running on any machine in a couple of minutes. Then, you can complete daily container tasks such as starting and stopping the container engine, building images, or interacting with containers.&lt;/p&gt; &lt;p&gt;It's okay if all you want to do for now is install your container engine, view the status, or manage your Kubernetes environment in your system toolbar. Podman Desktop can also start and stop the container engine from here or switch your Kubernetes context.&lt;/p&gt; &lt;p&gt;When you open the Podman Desktop UI, the dashboard displays the status of your container engines. You will also see familiar tabs for containers, images, pods, and volumes.&lt;/p&gt; &lt;p&gt;The following is a list of most of the basic container functions supported:&lt;/p&gt; &lt;h3&gt;Images and containers&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Pulling and building images&lt;/li&gt; &lt;li aria-level="1"&gt;Starting/stopping containers, configuring ports&lt;/li&gt; &lt;li aria-level="1"&gt;Inspecting containers logs&lt;/li&gt; &lt;li aria-level="1"&gt;Opening a terminal in containers and viewing log output&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Integration with registries&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Configure multiple OCI registries&lt;/li&gt; &lt;li aria-level="1"&gt;Authenticate to registries&lt;/li&gt; &lt;li aria-level="1"&gt;Pull, tag, and push images to your registries&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Volumes&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Browse and Inspect volumes created&lt;/li&gt; &lt;li aria-level="1"&gt;Configure proxies and Internal VPN&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Working with pods on Kubernetes&lt;/h2&gt; &lt;p&gt;Podman provides the capability to run pods in your local environment in a lightweight fashion. This enables you to easily work with Kubernetes objects directly in your developer environment without the need to spin up a local Kubernetes cluster. Watch this short videoillustrating the creation of a pod:&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt;Podman Desktop helps you gradually move from individual containers to pods with multiple containers and Kubernetes. You will find features enabling you to transition containers into pods, generate Kubernetes YAML, and deploy them on Kubernetes environments. Watch this quick video illustrating the deployment of a pod on Kubernetes:&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt;Kubernetes features include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Running pods locally with Podman&lt;/li&gt; &lt;li&gt;Creating pods from existing containers&lt;/li&gt; &lt;li&gt;Generating Kubernetes YAML&lt;/li&gt; &lt;li&gt;Deploying local pods to Kubernetes environments&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Future improvements to Podman Desktop&lt;/h2&gt; &lt;p&gt;In the coming months, we will explore moving from containers to pods to Kubernetes.&lt;/p&gt; &lt;p&gt;Even with the theoretical portability of containers, getting them to run on Kubernetes can be challenging. While more developers are exposed to Kubernetes (and Kubernetes YAML), they need to test the application in an environment more representative of production. Developers should not be required to understand the complexity of Kubernetes.&lt;/p&gt; &lt;p&gt;Podman Desktop will continue to enable developers to quickly develop their applications, provide fast turnarounds, and make it easier to test applications on Kubernetes environments.&lt;/p&gt; &lt;h2&gt;Resources for Podman Desktop&lt;/h2&gt; &lt;p&gt;Ready to try it out for yourself? &lt;a href="https://podman-desktop.io/"&gt;Podman Desktop&lt;/a&gt; installers are available on &lt;a href="https://podman-desktop.io/downloads"&gt;Windows, macOS, and Linux&lt;/a&gt;. Or you can use your favorite package manager, such as Brew, Flathub, Chocolatey, Scoop, or Windows Package Manager.&lt;/p&gt; &lt;p&gt;Learn more about Podman Desktop by visiting the &lt;a href="https://developers.redhat.com/search?t=podman&amp;s=most-recent"&gt;Red Hat Developer Podman&lt;/a&gt; web page and the Podman Desktop &lt;a href="https://github.com/containers/podman-desktop"&gt;GitHub repository&lt;/a&gt;. You can also join us on the &lt;a href="https://podman.io/"&gt;Podman site&lt;/a&gt; and &lt;a href="https://github.com/containers/podman"&gt;repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Get early access to the &lt;a href="https://developers.redhat.com/e-books/podman-action-early-access"&gt;Podman in Action e-book&lt;/a&gt; written by Daniel Walsh. Daniel leads the Podman team at Red Hat. Discover easy-to-follow examples to learn Podman quickly, including steps to deploy a complete containerized web service.&lt;/p&gt; &lt;p&gt;Feel free to comment below. We would love to hear where you think this project should go. Please report bugs or share ideas for enhancements on GitHub. We also welcome you to join our feedback program or Discord chat.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/24/podman-expands-desktop" title="Podman expands to the Desktop"&gt;Podman expands to the Desktop&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Stevan Le Meur, Tim deBoer</dc:creator><dc:date>2022-10-24T20:59:37Z</dc:date></entry><entry><title>How odo 3.0 GA improves the developer experience</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/24/odo-v3-ga-improves-developer-experience" /><author><name>Serena Chechile Nichols</name></author><id>46afa612-02b2-4116-97fd-9dcd118a4ca9</id><updated>2022-10-24T17:30:00Z</updated><published>2022-10-24T17:30:00Z</published><summary type="html">&lt;p&gt;Developers love command-line productivity. We get that. This article describes how &lt;strong&gt;odo&lt;/strong&gt;, a developer-focused command-line interface (CLI) for Red Hat OpenShift and Kubernetes, simplifies cloud-native development.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;odo&lt;/strong&gt; supports fast, iterative development and lets developers focus on what's most important to them—code. The odo 3.0 GA release provides new and improved user experience and security. This new release also allows developers to automatically detect bindable resources, making it easier to connect applications to services.&lt;/p&gt; &lt;h2&gt;6 ways odo v3 GA improves the developer experience&lt;/h2&gt; &lt;p&gt;The following sections describe six odo v3 GA features and commands that improve the developer experience.&lt;/p&gt; &lt;h3&gt;1. Interactive commands&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;odo&lt;/strong&gt; has new interactive commands to help developers with the discoverability of capabilities. Many odo commands provide an interactive and direct mode. In interactive mode, enter the command without options, and then you get a prompt for responses. In direct mode, enter the full command, including options, and there are no prompts provided.&lt;/p&gt; &lt;h3&gt;2. The odo init command&lt;/h3&gt; &lt;p&gt;Now, developers can use &lt;code&gt;odo init&lt;/code&gt;. V3 can auto-detect your source code and inject the appropriate language/framework template. Alternatively, you can begin with a starter project quickly.&lt;/p&gt; &lt;p&gt;&lt;img height="254" src="https://lh4.googleusercontent.com/SO9o-IjhWRotBRLcIajHkphJpIvY12XlrRa2yPjlvZqn0eSaqtKL4Ry6zA14A30YHzE29XsnVK7Vsc5NDkFGI9vJkEt-VEOOz2ZnzzqBAazBpJ7HQOnKI0ZRK1l5pfh8Nv9eYSe3MQbLUrNy5-RlfqzZHPpTaFA3KwM6ggKdj6vftC0X-r1Gq_DMSA" width="622" /&gt;&lt;/p&gt; &lt;h3&gt;3. The odo dev command&lt;/h3&gt; &lt;p&gt;Start development on your application with &lt;code&gt;odo dev&lt;/code&gt; to deploy the app to the cluster in dev mode. This command watches for changes in your local folder and automatically redeploys them to your cluster, allowing you to see real-time updates as you code.&lt;/p&gt; &lt;p&gt;&lt;code&gt;odo dev&lt;/code&gt; runs in the foreground until the user hits &lt;strong&gt;Ctrl+C&lt;/strong&gt;. It continuously watches the directory for any new changes (including the changes occurring in the Devfile) and automatically syncs them with the application running on the cluster.&lt;/p&gt; &lt;h3&gt;4. The odo deploy command&lt;/h3&gt; &lt;p&gt;Run the outer loop of your development cycle with &lt;code&gt;odo deploy&lt;/code&gt;. Use &lt;code&gt;odo deploy&lt;/code&gt; for the following scenarios:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You are working in a production environment.&lt;/li&gt; &lt;li&gt;The application is ready for public viewing.&lt;/li&gt; &lt;li&gt;You require building and pushing the container.&lt;/li&gt; &lt;li&gt;You need custom Kubernetes YAML for your production environment.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;5. The odo add binding command&lt;/h3&gt; &lt;p&gt;Use &lt;code&gt;odo add binding&lt;/code&gt; to connect to an operator-backed service in any namespace. This command has an interactive mode that lists bindable services across all the namespaces and a way to customize the related configuration, providing a fast and easy experience for connecting to a service. Currently, the bindable services listed are operator-backed services that support binding via the service binding operator. To learn more about the operators supported by the service binding operator, refer to its &lt;a href="https://github.com/redhat-developer/service-binding-operator#known-bindable-operators"&gt;README&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;6. odo works with OpenShift and Kubernetes&lt;/h3&gt; &lt;p&gt;Last but not least, &lt;strong&gt;odo&lt;/strong&gt; works with OpenShift or any Kubernetes cluster. You pick! We have even added aliases to match the mental models that worked with OpenShift or Kubernetes.&lt;/p&gt; &lt;p&gt;OpenShift developers can use the following commands:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;odo create project&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;odo list project&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;odo set project&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;odo delete project&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following commands may resonate better on Kubernetes:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;odo create namespace&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;odo list namespace&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;odo set namespace&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;odo delete namespace&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Ready to try odo v3?&lt;/h2&gt; &lt;p&gt;Head over to &lt;a href="https://odo.dev"&gt;odo.dev&lt;/a&gt;. Use our&lt;a href="https://odo.dev/docs/overview/installation/"&gt; installation guide&lt;/a&gt; and give it a try! Do you want to deploy your app to a cluster? Get free access with &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;OpenShift Toolkit for IDEs on Visual Studio Code and IntelliJ will soon support these new odo development workflows. Stay tuned for updates later this month.&lt;/p&gt; &lt;p&gt;For more information about &lt;strong&gt;odo&lt;/strong&gt; v3 GA, check out the following resources:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Watch the &lt;strong&gt;odo&lt;/strong&gt; v3 video tutorial.&lt;/li&gt; &lt;li&gt;Read the &lt;a href="http://odo.dev/blog/odo-v3-release"&gt;odo v3 GA release announcement&lt;/a&gt; on odo.dev.&lt;/li&gt; &lt;li&gt;Bookmark our odo v3 cheat sheet.&lt;/li&gt; &lt;li&gt;&lt;a href="https://odo.dev/docs/user-guides/v3-migration-guide/#commands-added-modified-or-removed-in-v3"&gt;Learn more&lt;/a&gt; about the commands added, modified, or removed in the v3 release.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/24/odo-v3-ga-improves-developer-experience" title="How odo 3.0 GA improves the developer experience"&gt;How odo 3.0 GA improves the developer experience&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Serena Chechile Nichols</dc:creator><dc:date>2022-10-24T17:30:00Z</dc:date></entry><entry><title>Red Hat joins the Backstage.io community</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/24/red-hat-joins-backstageio-community" /><author><name>Serena Chechile Nichols</name></author><id>0c8df3ef-d56f-4f8b-bbc8-180dac42e289</id><updated>2022-10-24T16:00:00Z</updated><published>2022-10-24T16:00:00Z</published><summary type="html">&lt;p&gt;The concept of platform engineering and the end-to-end developer experience is a burgeoning topic industry wide. Building an IdP (Internal Developer Portal) is extremely complex. This topic is new for many, and there are still a lot of unknowns regarding how to evolve an organization that has no, or a low, concept of internal platforms.&lt;/p&gt; &lt;p&gt;Enter &lt;a href="https://www.backstage.io"&gt;Backstage&lt;/a&gt;. Backstage is an &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="33b2d59a-e8f1-4509-8554-d86684d539b0" href="https://developers.redhat.com/topics/open-source" title="Open Source for Developers"&gt;open source&lt;/a&gt; framework for building developer portals donated to the Cloud Native Computing Foundation by Spotify. Backstage has a vibrant ecosystem that development teams successfully use to streamline and rapidly onboard applications. It provides a portal into an internal developer platform by delivering an application catalog that can aggregate several sources of information regarding applications.&lt;/p&gt; &lt;p&gt;Backstage is becoming a standard for developer scaffolding. Building this type of platform to fit into your environment is both complex and time consuming. Knowledge around Backstage is still hard to find. Organizations are looking for a standardized approach on how to implement and adopt Backstage. We have seen an increased interest in Backstage by our Red Hat customers. We have a number of consulting engagements targeting building IdPs and implementing Backstage, which will allow Red Hat to accumulate significant expertise on how to best guide when building their developer platforms.&lt;/p&gt; &lt;p&gt;We believe that developers are force multipliers, and their productivity correlates with the ability of organizations to generate revenue, launch new features, and capitalize on opportunities. But navigating the complexity of a modern Kubernetes-native ecosystem is hard. They need the best tools to have access not only to &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;APIs&lt;/a&gt;, but also to documentation and content, software development kits (SDKs), debugging tools, monitoring and tracing tools, and user interfaces that help them with their tasks at hand.&lt;/p&gt; &lt;p&gt;Red Hat today announces that it will join the backstage.io community. We will bring our own expertise and customer experiences together with a community of partners and open source projects to improve developer experience on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. OpenShift and &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="29094412-2efe-475d-9743-e6e4470c3599" href="https://developers.redhat.com/topics/kubernetes" title="Developing applications on Kubernetes"&gt;Kubernetes&lt;/a&gt; are critical to our customers, and we want to provide developer tools that make it easier and more productive for your teams to build services and applications along with lowering the cognitive burden of navigating a Kubernetes-native application development world.&lt;/p&gt; &lt;p&gt;With Red Hat’s experience in building, contributing to, and expanding open source projects into industry standards supported by development resources and leadership, we hope to make significant contributions to backstage.io for the advancement of its industry adoption.&lt;/p&gt; &lt;p&gt;As a first step, we are making available a &lt;a href="https://github.com/redhat-developer/helm-backstage"&gt;Helm chart&lt;/a&gt; that gets Backstage up and running in both an OpenShift deployment as well as a vanilla K8s environment. What starts with a small step is the beginning of the adventure of bringing Backstage into the OpenShift ecosystem and on a collaborative path to an enterprise-supported developer platform for OpenShift developers.&lt;/p&gt; &lt;p&gt;We invite you to help the community to enhance developer productivity and create the best developer experience for the next generation of developers. Subscribe to the &lt;a href="https://discordapp.com/channels/687207715902193673/995973463208644678"&gt;Backstage Discord deployment channel&lt;/a&gt; for community discussions around our new Helm chart contribution. Stay tuned for additional blog posts and information to be released soon!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/24/red-hat-joins-backstageio-community" title="Red Hat joins the Backstage.io community"&gt;Red Hat joins the Backstage.io community&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Serena Chechile Nichols</dc:creator><dc:date>2022-10-24T16:00:00Z</dc:date></entry><entry><title>Use design by contract to build Kubernetes Operators in Java</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/24/use-design-contract-build-kubernetes-operators-java" /><author><name>Sun Tan, Andrea Peruffo</name></author><id>30c01f48-9df4-4f63-a9ec-330f7b539bee</id><updated>2022-10-24T07:00:00Z</updated><published>2022-10-24T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; is great at automating the deployment, monitoring, and scaling of applications. Whatever you want Kubernetes to control—a service, deployment, etc.—is called a &lt;em&gt;resource&lt;/em&gt;. This article shows you how to extend Kubernetes's capabilities by writing a Custom Resource Definition (CRD) and an Operator in &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;CRDs specify how to install, monitor, and manage resources, whereas Operators are agents that actually carry all that out. The usual language for writing an Operator is &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;. But most Java teams prefer to keep all their code in Java: they want to avoid doing a deep dive into a new language, and to maintain a unified environment without supporting two sets of tools and practices.&lt;/p&gt; &lt;p&gt;This article explains how to create a CRD and Operator in Java, complete with sample Java code and YAML specification files. We'll use the &lt;a href="https://github.com/java-operator-sdk/"&gt;Java Operator SDK&lt;/a&gt;, the &lt;a href="https://github.com/fabric8io/kubernetes-client"&gt;Fabric8 Kubernetes Java client&lt;/a&gt;, and a brand new CRD-to-Java mapping generator.&lt;/p&gt; &lt;p&gt;If you'd like to extend Kubernetes with Java, you are in the right place. If you are just curious about the concepts described so far, you are also welcome to continue.&lt;/p&gt; &lt;p&gt;You can find the code sample used in this article in &lt;a href="https://github.com/sunix/jdevspace"&gt;the jdevspace Github project&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What is a Custom Resource Definition?&lt;/h2&gt; &lt;p&gt;When deploying an application to a Kubernetes cluster, you create a &lt;code&gt;Deployment&lt;/code&gt; resource and Kubernetes auto-magically starts your pods and containers in your cluster. The magic is actually executed through &lt;em&gt;controllers&lt;/em&gt;, which are processes run by Kubernetes to track the creation and updates of resources. A controller executes the business logic defined by the developer in the resources' specifications, which are normally written in YAML. Controllers can create new pods or perform any other action requested by the developer.&lt;/p&gt; &lt;p&gt;Usually, each controller is in charge of tracking specific kinds of resources. Kubernetes provides built-in resources and built-in controllers. But you can install custom resources and custom controllers on top of Kubernetes. The custom controller is typically provided by an Operator.&lt;/p&gt; &lt;p&gt;Your custom resource (CR) works the same way as the built-in resources. Kubernetes, thanks to its custom controller, auto-magically starts pods or containers or does whatever you say your application needs.&lt;/p&gt; &lt;p&gt;The custom resources are usually defined in Custom Resource Definition (CRD) files that provide schemas written in YAML and describe the structure of a custom resource type. The CRD specifies the desired state of a resource. The custom controller has the responsibility of applying the changes requested by the CRD to the cluster or even outside the cluster.&lt;/p&gt; &lt;p&gt;A CRD can then be registered in the cluster, usually identified by a name. The developer creates a custom resource based on a registered CRD type.&lt;/p&gt; &lt;p&gt;Each custom resource is composed of a &lt;code&gt;spec&lt;/code&gt; section written by the developer and a &lt;code&gt;status&lt;/code&gt; section updated at runtime by the custom controller to reflect the resource's status or the result of the CRD's actions.&lt;/p&gt; &lt;p&gt;Figure 1 shows how the whole system fits together in a Kubernetes cluster.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/karch.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/karch.png?itok=2ZB6rSBA" width="911" height="484" alt="A controller in Kubernetes reads the developer's specs and maintains resources in the requested state." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A controller in Kubernetes reads the developer's specs and maintains resources in the requested state. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;CRDs and Operators&lt;/h2&gt; &lt;p&gt;As the previous section has shown, Kubernetes is highly customizable. It is possible to extend Kubernetes with CRDs and custom controllers, which in turn are usually bundled into a Kubernetes Operator.&lt;/p&gt; &lt;p&gt;The CRD is the key part of an Operator. The CRD defines the contract between the developer and the Operator—that is, it lays out what a developer can provide to the Operator and what information the Operator returns to the developer.&lt;/p&gt; &lt;p&gt;There are many advantages to a "design by contract" approach in which CRDs are specified independently from the controller implementations. This division of labor makes a system more flexible, interoperable, and modular. CRDs can be used not only by controllers, but also by graphical user interfaces, command-line tools, and other resources.&lt;/p&gt; &lt;h2&gt;The developer workspace application&lt;/h2&gt; &lt;p&gt;In this article, our goal is to create a custom controller that tracks changes and updates custom resources. For our demo example, I have chosen the DevWorkspace CRD, specified as part of the &lt;a href="https://github.com/devfile/api"&gt;Devfile API&lt;/a&gt;. There is already a &lt;a href="https://github.com/devfile/devworkspace-operator"&gt;DevWorkspace Operator&lt;/a&gt;, written in Go. In this article, we will not simply rewrite the Go Operator in Java, but instead will build a fresh Java Operator using the "design by contract" approach.&lt;/p&gt; &lt;p&gt;The DevWorkspace Operator and the Devfile API come from the &lt;a href="https://github.com/eclipse/che"&gt;Eclipse Che project&lt;/a&gt;. These Operators allow developers to generate on-demand developer environments in a Kubernetes cluster with a Kubernetes resource that describes a workspace. When a developer creates a &lt;code&gt;DevWorkspace&lt;/code&gt; resource in a cluster, the DevWorkspace Operator starts a pod, clones the project, and starts containers for the developer environment, including a web IDE.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;DevWorkspace&lt;/code&gt; resource contains information such as the projects to clone, the containers that build and run your cloud-native application, the Web IDE, etc. More information about the DevWorkspace Operator can be found in &lt;a href="https://developers.redhat.com/articles/2022/04/01/codeready-workspaces-scales-now-red-hat-openshift-dev-spaces"&gt;CodeReady Workspaces scales up, is now Red Hat OpenShift Dev Spaces&lt;/a&gt;, an article on Red Hat Developer from Mario Loriedo.&lt;/p&gt; &lt;p&gt;In this article, you will register the DevWorkspace CRDs and start a custom controller in &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; development mode. The custom controller doesn't implement a complete DevWorkspace controller, but demonstrates how to perform the basic operations a controller should be able to do on creation or update of a &lt;code&gt;DevWorspace&lt;/code&gt; resource:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Read the content from the &lt;code&gt;DevWorkspace&lt;/code&gt; resource &lt;code&gt;spec&lt;/code&gt; section.&lt;/li&gt; &lt;li&gt;Update the &lt;code&gt;status&lt;/code&gt; section of the &lt;code&gt;DevWorkspace&lt;/code&gt; resource.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Registering the Java CRD&lt;/h2&gt; &lt;p&gt;To start, set up your development cluster. To work on a custom controller, the CRDs have to be registered in your cluster. Note that this step would be automated for downstream developers during the installation phase of the Operator.&lt;/p&gt; &lt;p&gt;To register the CRDs on my freshly installed &lt;a href="https://kubernetes.io/docs/tasks/tools/#minikube"&gt;minikube&lt;/a&gt; installation, I run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -f https://github.com/devfile/api/raw/main/crds/workspace.devfile.io_devworkspacetemplates.yaml $ kubectl apply -f https://github.com/devfile/api/raw/main/crds/workspace.devfile.io_devworkspaces.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After you have the CRDs on your cluster, you should be able to create a new &lt;code&gt;DevWorkspace&lt;/code&gt; resource based on the corresponding CRD as follows.&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create a new file named &lt;code&gt;mydevworkspace.yaml&lt;/code&gt; with the following content: &lt;pre&gt; &lt;code&gt;kind: "DevWorkspace" apiVersion: "workspace.devfile.io/v1alpha2" metadata: name: mydevworkspace spec: started: true routingClass: 'openshift-auth' template: projects: - name: "devworkspace-spec" git: remotes: origin: "https://github.com/che-incubator/devworkspace-api" checkoutFrom: revision: "master"&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create the custom resource in the cluster: &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -f mydevworkspace.yaml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Display the created custom resource: &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl get DevWorkspace -o yaml&lt;/code&gt; apiVersion: v1 items: - apiVersion: workspace.devfile.io/v1alpha2 kind: DevWorkspace metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {"apiVersion":"workspace.devfile.io/v1alpha2","kind":"DevWorkspace","metadata":{"annotations":{},"name":"mydevworkspace","namespace":"default"},"spec":{"routingClass":"openshift-auth","started":true,"template":{"projects":[{"git":{"checkoutFrom":{"revision":"master"},"remotes":{"origin":"https://github.com/che-incubator/devworkspace-api"}},"name":"devworkspace-spec"}]}}} creationTimestamp: "2022-09-15T08:28:54Z" generation: 1 name: mydevworkspace namespace: default resourceVersion: "111514" uid: b8d37753-1352-4e55-9d02-143ef2b72004 spec: routingClass: openshift-auth started: true template: projects: - git: checkoutFrom: revision: master remotes: origin: https://github.com/che-incubator/devworkspace-api name: devworkspace-spec kind: List metadata: resourceVersion: ""&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The output of the previous command shows that the &lt;code&gt;DevWorkspace&lt;/code&gt; resource has been created, but nothing is happening. The created resource looks like the one you defined in the &lt;code&gt;mydevworkspace.yaml&lt;/code&gt; file., but some annotations have been added and there is no &lt;code&gt;status&lt;/code&gt; section. This result is normal, because you don't have any controller installed for that kind of resource.&lt;/p&gt; &lt;p&gt;So your next step is to create a custom controller. The controller registers the CR creation and modification events. It then acts according to the CR specifications. Finally, it updates the CR &lt;code&gt;status&lt;/code&gt; section.&lt;/p&gt; &lt;h3&gt;The Java Operator SDK&lt;/h3&gt; &lt;p&gt;Normally, a custom controller is created using the Kubernetes &lt;a href="https://sdk.operatorframework.io/build/"&gt;Operator SDK&lt;/a&gt;. There are several ways to achieve this using Go, Helm, or Ansible. But none of these methods is easy for developers coming from the Java world. It should be possible to run the controller in any language because the controller interacts with Kubernetes through a &lt;a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/"&gt;REST API&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In the Java landscape, the &lt;a href="https://github.com/fabric8io/kubernetes-client"&gt;Fabric8 Kubernetes Java client&lt;/a&gt; is the best Java interface to the Kubernetes API. However, that client offers a quite low-level API, so you would have to implement a lot of plumbing.&lt;/p&gt; &lt;p&gt;An important advance is described in the excellent article &lt;a href="https://developers.redhat.com/articles/2022/03/22/write-kubernetes-java-java-operator-sdk-part-2#implement_the_controller"&gt;Write Kubernetes in Java with the Java Operator SDK&lt;/a&gt; by my colleague at Red Hat, &lt;a href="https://twitter.com/metacosm"&gt;Chris Laprun&lt;/a&gt;. His team is working on the Java Operator SDK, which is the equivalent of the default Go-based Operator SDK. The Java Operator SDK uses the Fabric8 Kubernetes Java client behind the scenes and should help you write your custom Operator very easily, with style, and in Java. Let's give it a try.&lt;/p&gt; &lt;h3&gt;Your first custom controller in Java&lt;/h3&gt; &lt;p&gt;After &lt;a href="https://sdk.operatorframework.io/docs/installation/"&gt;installing the Operator SDK&lt;/a&gt;, let's bootstrap a new project as described in Chris's article. In the following examples, I take the version &lt;code&gt;v1alpha1&lt;/code&gt; and the domain &lt;code&gt;workspace.devfile.io&lt;/code&gt; from the DevWorkspace CRD files.&lt;/p&gt; &lt;p&gt;Generate the project skeleton by using the Operator SDK Quarkus plugin: &lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ operator-sdk init --plugins quarkus --domain workspace.devfile.io --project-name jdevspace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create new &lt;code&gt;DevWorkspace&lt;/code&gt; resource Java types:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ operator-sdk create api --version v1alpha1 --kind DevWorkspace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;span&gt; &lt;/span&gt;The directory structure of our new &lt;code&gt;DevWorkspace&lt;/code&gt; Java Operator project should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;. ├── Makefile ├── pom.xml ├── PROJECT └── src └── main ├── java │ └── jdevspace │ └── eclipse │ └── org │ ├── DevWorkspace.java │ ├── DevWorkspaceReconciler.java │ ├── DevWorkspaceSpec.java │ └── DevWorkspaceStatus.java └── resources └── application.properties 7 directories, 8 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should now take the special step of disabling generation of the container image, because you don't need it during development. Simply set &lt;code&gt;quarkus.container-image.build&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; in &lt;code&gt;src/main/resources/application.properties&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now edit the &lt;code&gt;DevWorkspaceReconciler.java&lt;/code&gt; file. The reconciler object is the core part of the controller, responsible for enforcing the desired state in the custom resource state on the actual state of the system. The &lt;code&gt;reconcile&lt;/code&gt; method is triggered each time a modification or creation of a Custom Resource is invoked. For now, include just a trivial method that prints the name of the custom resource to your terminal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package io.devfile.workspace; import io.fabric8.kubernetes.client.KubernetesClient; import io.javaoperatorsdk.operator.api.reconciler.Context; import io.javaoperatorsdk.operator.api.reconciler.Reconciler; import io.javaoperatorsdk.operator.api.reconciler.UpdateControl; public class DevWorkspaceReconciler implements Reconciler&lt;DevWorkspace&gt; { private final KubernetesClient client; public DevWorkspaceReconciler(KubernetesClient client) { this.client = client; } @Override public UpdateControl&lt;DevWorkspace&gt; reconcile(DevWorkspace resource, Context context) { System.out.println("Hello " + resource.getMetadata().getName()); return UpdateControl.noUpdate(); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To give the controller a try, start Quarkus in dev mode. The runtime should run your controller locally and use your existing &lt;code&gt;kubectl&lt;/code&gt; session to interact with the Kubernetes API. (In the real world, the controller would run in its own pod and container.)&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn quarkus:dev&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Unfortunately, at this point you'll encounter an exception at startup:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Caused by: com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field "routingClass" (class io.devfile.workspace.DevWorkspaceSpec), not marked as ignorable (0 known properties: ])&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This error arises because the CRD requires a &lt;code&gt;routingClass&lt;/code&gt; field that is not implemented in your CRD Java mapping. We will fix this problem later on.&lt;/p&gt; &lt;p&gt;To proceed in the absence of the solutions, load an empty &lt;code&gt;DevWorkspace&lt;/code&gt; resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# deleting the existing DevWorkspace resource kubectl delete DevWorkspace mydevworkspace # creating a new one with just the name cat &lt;&lt;EOF | kubectl apply -f - kind: "DevWorkspace" apiVersion: "workspace.devfile.io/v1alpha2" metadata: name: simpledevworkspace EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Restart Quarkus dev mode by pressing the Space key in the appropriate terminal. You should see output like the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;__ ____ __ _____ ___ __ ____ ______ --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,&lt; / /_/ /\ \ --\___\_\____/_/ |_/_/|_/_/|_|\____/___/ 2022-07-11 14:46:45,365 INFO [io.jav.ope.Operator] (Quarkus Main Thread) Registered reconciler: 'devworkspacereconciler' for resource: 'class io.devfile.workspace.DevWorkspace' for namespace(s): [all namespaces] 2022-07-11 14:46:45,365 INFO [io.qua.ope.run.AppEventListener] (Quarkus Main Thread) Quarkus Java Operator SDK extension 3.0.7 (commit: 22fed83 on branch: 22fed8391b7b153616bd79c5f829cdd8a7edd5bd) built on Thu Apr 07 16:13:21 CEST 2022 2022-07-11 14:46:45,366 INFO [io.jav.ope.Operator] (Quarkus Main Thread) Operator SDK 2.1.4 (commit: 5af3fec) built on Thu Apr 07 10:31:06 CEST 2022 starting... 2022-07-11 14:46:45,366 INFO [io.jav.ope.Operator] (Quarkus Main Thread) Client version: 5.12.2 2022-07-11 14:46:45,517 INFO [io.quarkus] (Quarkus Main Thread) jdevspace 0.0.1-SNAPSHOT on JVM (powered by Quarkus 2.7.5.Final) started in 0.725s. Listening on: http://localhost:8080 2022-07-11 14:46:45,518 INFO [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated. 2022-07-11 14:46:45,519 INFO [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kubernetes, kubernetes-client, micrometer, openshift-client, operator-sdk, smallrye-context-propagation, smallrye-health, vertx] 2022-07-11 14:46:45,519 INFO [io.qua.dep.dev.RuntimeUpdatesProcessor] (Aesh InputStream Reader) Live reload total time: 0.758s Hello simpledevworkspace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;Hello simpledevworkspace&lt;/code&gt; message is logged from our controller, showing that it is working.&lt;/p&gt; &lt;h2&gt;CRD-to-Java type mapping with the Fabric8 Kubernetes client&lt;/h2&gt; &lt;p&gt;Your controller is currently working with an empty &lt;code&gt;DevWorkspace&lt;/code&gt; resource. Now you face a big job: To implement the  &lt;code&gt;DevWorkspace&lt;/code&gt; resource.&lt;/p&gt; &lt;p&gt;Done manually, this task is enormous. For instance, to support just the &lt;code&gt;isStarted&lt;/code&gt; field of the specification, your &lt;code&gt;DevWorkspaceSpec&lt;/code&gt; class would look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package io.devfile.workspace; public class DevWorkspaceSpec { private Boolean started; public Boolean getStarted() { return started; } public void setStarted(Boolean started) { this.started = started; } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It looks like it would be too complicated to implement the whole Java class mapping for your custom resource. Luckily, there are a few options to cut down on development effort.&lt;/p&gt; &lt;p&gt;One option is to create a &lt;code&gt;fabric8-kubernetes-client&lt;/code&gt; extension. An extension consists of a Go script that converts the CRD types to Java. Each extension would generate its own Maven artifact to be used in the project. Some examples can be found in the &lt;a href="https://github.com/fabric8io/kubernetes-client/tree/v6.0.0/extensions"&gt;Kubernetes client extensions repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article doesn't pursue this option; remember, we've decided that we don't want to write or maintain any Go code. Furthermore, if your CRD is already implemented as an extension in &lt;code&gt;fabric8-kubernetes-client&lt;/code&gt;, you would need to follow the release cadence to get changes.&lt;/p&gt; &lt;p&gt;The option adopted in this article is to use the very new CRD-to-Java type generator in the 6.0.0 release of the Fabric8 Kubernetes client. Kudos to Andrea Peruffo for this amazing contribution. The 6.0.0 version has just been released, and the CRD-to-Java type generator is just one of its great features. You can learn more about these features in the article &lt;a href="https://developers.redhat.com/articles/2022/07/15/new-http-clients-java-generator-and-more-fabric8-600"&gt;New HTTP clients, a Java generator, and more in Fabric8 6.0.0&lt;/a&gt; by Steven Hawkins.&lt;/p&gt; &lt;p&gt;This article deploys the new CRD-to-Java type generator as a Maven plugin and attaches it to the &lt;code&gt;generate-sources&lt;/code&gt; phase of your project to auto-magically generate the Java types for your CRD files.&lt;/p&gt; &lt;h3&gt;The CRD-to-Java type generator in action&lt;/h3&gt; &lt;p&gt;In this section, we follow the &lt;a href="https://github.com/fabric8io/kubernetes-client/blob/v6.0.0/doc/java-generation-from-CRD.md"&gt;guide provided&lt;/a&gt; to generate the Java types from the &lt;code&gt;DevWorkspace&lt;/code&gt; CRD files.&lt;/p&gt; &lt;p&gt;Download the &lt;code&gt;DevWorkspace&lt;/code&gt; CRD files to &lt;code&gt;src/main/resources/kubernetes&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir src/main/resources/kubernetes $ cd src/main/resources/kubernetes $ wget https://github.com/devfile/api/raw/main/crds/workspace.devfile.io_devworkspacetemplates.yaml $ wget https://github.com/devfile/api/raw/main/crds/workspace.devfile.io_devworkspaces.yaml $ cd -&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the generator Maven plugin to the project's &lt;code&gt;build/plugins&lt;/code&gt; section of &lt;code&gt;pom.xml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt; &lt;plugin&gt; &lt;groupId&gt;io.fabric8&lt;/groupId&gt; &lt;artifactId&gt;java-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;source&gt;src/main/resources/kubernetes&lt;/source&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;!-- a hint for IDE's to add the java sources to the classpath --&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;build-helper-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;generate-sources-config&lt;/id&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;sources&gt; &lt;source&gt;${project.build.directory}/generated-sources/java/&lt;/source&gt; &lt;/sources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also list the plugin in the dependencies:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt; &lt;dependency&gt; &lt;groupId&gt;javax.validation&lt;/groupId&gt; &lt;artifactId&gt;validation-api&lt;/artifactId&gt; &lt;version&gt;2.0.1.Final&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now run one of the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn java-generator:generate $ mvn clean package&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see the appropriate files generated in the &lt;code&gt;target/generated-sources/java&lt;/code&gt; folder. A subset of the generated files is shown in the following tree:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;target/generated-sources/java/io/devfile/workspace/v1alpha1/ ├── DevWorkspace.java ├── devworkspacespec │ ├── template │ └── Template.java ├── DevWorkspaceSpec.java ├── devworkspacestatus │ └── Conditions.java ├── DevWorkspaceStatus.java ├── DevWorkspaceTemplate.java ├── devworkspacetemplatespec │ ├── commands │ ├── Commands.java │ ├── components │ ├── Components.java │ ├── Events.java │ ├── parent │ ├── Parent.java │ ├── projects │ ├── Projects.java │ ├── starterprojects │ └── StarterProjects.java └── DevWorkspaceTemplateSpec.java 9 directories, 13 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The extent of generated files is enormous. The procedure in this section is certainly a big improvement over creating the whole CRD-to-Java mapping manually.&lt;/p&gt; &lt;h3&gt;Custom controller with the generated Java types&lt;/h3&gt; &lt;p&gt;Now that you have generated Java types from the CRDs, you no longer need the types created initially, so you can remove the &lt;code&gt;DevWorkspace&lt;/code&gt;, &lt;code&gt;DevWorkspaceSpec&lt;/code&gt;, and &lt;code&gt;DevWorkspaceStatus&lt;/code&gt; Java files from &lt;code&gt;src/main/java&lt;/code&gt;. You can also update the &lt;code&gt;DevWorkspaceReconciler&lt;/code&gt; class to use the newly generated &lt;code&gt;DevWorkspace&lt;/code&gt; classes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package io.devfile.workspace; import io.devfile.workspace.v1alpha1.DevWorkspace; &lt;!-- importing the generated class --&gt; import io.fabric8.kubernetes.client.KubernetesClient; import io.javaoperatorsdk.operator.api.reconciler.Context; import io.javaoperatorsdk.operator.api.reconciler.Reconciler; import io.javaoperatorsdk.operator.api.reconciler.UpdateControl; public class DevWorkspaceReconciler implements Reconciler&lt;DevWorkspace&gt; { private final KubernetesClient client; public DevWorkspaceReconciler(KubernetesClient client) { this.client = client; } @Override public UpdateControl&lt;DevWorkspace&gt; reconcile(DevWorkspace resource, Context context) { System.out.println("Hello " + resource.getMetadata().getName() + " started: " + resource.getSpec().getStarted()); return UpdateControl.noUpdate(); } }&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Reconciling a complete custom resource&lt;/h3&gt; &lt;p&gt;Start Quarkus dev mode again. The controller should work as previously, but you can now also create a complete &lt;code&gt;DevWorkspace&lt;/code&gt; resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOF | kubectl apply -f - kind: "DevWorkspace" apiVersion: "workspace.devfile.io/v1alpha2" metadata: name: mydevworkspace spec: started: true routingClass: 'openshift-auth' template: projects: - name: "devworkspace-spec" git: remotes: origin: "https://github.com/che-incubator/devworkspace-api" checkoutFrom: revision: "master" EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following message should be logged:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Hello mydevworkspace started: true&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;A custom controller in Java&lt;/h2&gt; &lt;p&gt;Remember, our goal in this article is to implement a custom controller for existing CRDs in Java. That controller should be able to read the resource's &lt;code&gt;spec&lt;/code&gt; section and write to its &lt;code&gt;status&lt;/code&gt; section. Update your controller accordingly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package io.devfile.workspace; import io.devfile.workspace.v1alpha1.DevWorkspace; import io.devfile.workspace.v1alpha1.DevWorkspaceStatus; import io.fabric8.kubernetes.client.KubernetesClient; import io.javaoperatorsdk.operator.api.reconciler.Context; import io.javaoperatorsdk.operator.api.reconciler.Reconciler; import io.javaoperatorsdk.operator.api.reconciler.UpdateControl; public class DevWorkspaceReconciler implements Reconciler&lt;DevWorkspace&gt; { private final KubernetesClient client; public DevWorkspaceReconciler(KubernetesClient client) { this.client = client; } @Override public UpdateControl&lt;DevWorkspace&gt; reconcile(DevWorkspace resource, Context context) { if (resource.getSpec().getTemplate() == null || resource.getSpec().getTemplate().getProjects().isEmpty()) { return UpdateControl.noUpdate(); } DevWorkspaceStatus status = new DevWorkspaceStatus(); status.setMessage( "Project source " + resource.getSpec().getTemplate().getProjects().get(0).getGit().getRemotes().get("origin")); status.setWorkspaceId("my workspace"); resource.setStatus(status); return UpdateControl.updateStatus(resource); } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This controller will:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Read the &lt;code&gt;spec&lt;/code&gt; section&lt;/li&gt; &lt;li&gt;Look for the project Git URL&lt;/li&gt; &lt;li&gt;Write the project Git URL in the resource &lt;code&gt;status&lt;/code&gt; section&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Once you save the file, Quarkus dev mode should automatically retrigger the controller and update the &lt;code&gt;DevWorkspace&lt;/code&gt; resources in your cluster. Check for success as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl get DevWorkspaces NAME DEVWORKSPACE ID PHASE INFO mydevworkspace Project source https://github.com/che-incubator/devworkspace-api started-devworkspace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This message shows that the controller works as expected. The output shows the project's Git URL, defined in the &lt;code&gt;spec&lt;/code&gt; session and logged in the &lt;code&gt;status&lt;/code&gt; of the same resource (&lt;code&gt;INFO&lt;/code&gt; is displaying the content of the &lt;code&gt;status&lt;/code&gt; section).&lt;/p&gt; &lt;h2&gt;Custom resource definitions and Operators can be written fairly easily in Java&lt;/h2&gt; &lt;p&gt;To sum up this article, we have written a Kubernetes Operator in Java from existing CRDs as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;We used the Java Operator SDK to write a custom controller.&lt;/li&gt; &lt;li&gt;Following a contract-first approach, we used the generator from the Fabric8 Kubernetes client to generate the Java types from existing CRDs.&lt;/li&gt; &lt;li&gt;We implemented a very basic controller with read and write capabilities on a custom resource.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;However, many aspects of a custom Operator weren't covered in this article. These tasks include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The Operator Lifecycle Manager (OLM), which extends Kubernetes to provide a declarative way to install, manage, and upgrade Operators on a cluster.&lt;/li&gt; &lt;li&gt;The Operator scope: Should it be covering the resource in a cluster scope or a namespace scope?&lt;/li&gt; &lt;li&gt;RBAC to manage permissions that an Operator can have on built-in resources or custom resources.&lt;/li&gt; &lt;li&gt;Starting pods, services, and so on through the DevWorkspace Operator. These functions could be added using the Fabric8 Kubernetes client.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Writing an Operator in Java with a contract-first approach is one of the use cases for the CRD-to-Java type generator. But there are other situations in which the generator could be useful, such as a remote command-line interface (CLI) to manage or configure an Operator, and a Java IDE plugin to browse and edit custom resources in a remote cluster.&lt;/p&gt; &lt;p&gt;We would love to get feedback from you. You can create an issue on the Github repos for the &lt;a href="https://github.com/fabric8io/kubernetes-client/issues/new/choose"&gt;CRD to Java generator&lt;/a&gt; or the &lt;a href="https://github.com/java-operator-sdk/java-operator-sdk/issues/new/choose"&gt;Java Operator SDK&lt;/a&gt;. You could also post a comment on this article or just let us know that you love our work in our community &lt;a href="https://gitter.im/fabric8io/kubernetes-client"&gt;Gitter channel&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/24/use-design-contract-build-kubernetes-operators-java" title="Use design by contract to build Kubernetes Operators in Java"&gt;Use design by contract to build Kubernetes Operators in Java&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Sun Tan, Andrea Peruffo</dc:creator><dc:date>2022-10-24T07:00:00Z</dc:date></entry><entry><title type="html">Kogito Service Discovery for Workflows</title><link rel="alternate" href="https://blog.kie.org/2022/10/kogito-service-discovery-for-workflows.html" /><author><name>Filippe Spolti</name></author><id>https://blog.kie.org/2022/10/kogito-service-discovery-for-workflows.html</id><updated>2022-10-21T12:25:58Z</updated><content type="html">I have recently been wondering: how many times do we find ourselves accessing a specific resource inside a Kubernetes cluster searching for information about how to access it or to expose our awesome service so others can consume it. Right? So, this enhanced service discovery might help you get rid of it by setting a specific static URI that holds all information that the service discovery engine needs to know to translate it to the right service backed by the corresponding Deployment, StatefulSet, Pod, etc. The service discovery engine, part of the Kogito project, is a Quakus add-on part of Kogito Serverless Workflow engine and for now it can be added to other Quarkus Applications, however it will bring the Kogito Serveriess Workflows dependencies. It is in our roadmap to make the Discovery engine a standalone add-on, allowing it to be added to any Quarkus application. It is expected that you are working with the Kogito Serverless and already have the kogito-quarkus-serverless-workflow extension in the application’s dependencies. For more information about Kogito Serverless Workflow . What’s the problem? OK, but what problem are we trying to solve here? Well, Kubernetes resources are dynamic and its configuration values can change from time to time, which can require some manual steps later to get the updated configuration, let’s say, by a cluster upgrade or a domain name change that can lead an ingress route being updated or a Knative endpoint being changed as well. The service discovery can help you with that by abstracting these changes from the user keeping the Application that consumes these services up to date. Instead of rebuilding the whole application so that the Quarkus properties can take effect, just restarting the application is enough. If the application is a Knative Service, then it can be automatically done by Knative if the service doesn’t receive requests for a while since it is scaled to 0 replicas. BUT, WHAT’S THE DIFFERENCE BETWEEN THE CURRENT SERVICE DISCOVERY THAT WE ALREADY HAVE OUT THERE? There are a few service discovery engines available. , for example, which is a great option to discover in a cluster (At the time of publication of this article, we are investigating the new Stork’s new feature that allows custom discovery). However, the Kogito team wanted to move one step forward and, instead of being able to look up only for services, why not be able to search for almost any kind of Kubernetes resources that helps expose the pod? That is where the Kogito Service discovery engine kicks in, with it you can easily discover the most common Kubernetes resources that expose your service or application. Take a look on the picture below that quickly demonstrates how exactly it works: Picture 1: Service Discovery flow In a nutshell, the engine scans the Quarkus configuration searching for any value that matches the URI that the engine expects. Once found, it queries the Kubernetes API searching for the given resource. Take a look in the diagram below:   Picture 2: URI scheme For the scheme, we have three options to help to better identify where the resource is running on: * kubernetes * openshift * knative About the Kubernetes resources, identified by the Group,Version and Kind (GVK), there is a list with a few of the most common objects that can be used for discovering: * v1/service * serving.knative.dev/v1/service * v1/pod * apps/v1/deployment * apps.openshift.io/v1/deploymentconfig * apps/v1/statefulset * route.openshift.io/v1/route * networking.k8s.io/v1/ingress &gt; Keep in mind that the GVK information is mandatory, however the &gt; namespace/project is optional and, if empty, the one where the application is &gt; running will be used. The  resource  to be queried is always the last item in &gt; the URI. HELPING THE SERVICE DISCOVERY ENGINE BE MORE PRECISE The engine allows users to set query strings that helps the discovery process to select the correct resource. The available parameters are: * Custom Labels: Can be used to filter services in cases where two different services that have the same Label Selector but expose different ports. In such cases, the custom label will be used to filter the right service. The custom label accepts multiple values separated by a semicolon. In the following example, we have two labels, app-port and app-revision: kubernetes:v1/service/test/my-service?labels=app-port=default;app-revision=10 * Port Name: A container can expose more than one port, in which case the engine might not return the expected port. The order of precedence of the engine is/looks like this: user defined port name -&gt; https -&gt; port named as http or web -&gt; first port in the list. The port-name can be defined to help the engine use the correct port for such cases where the precedence shown above is not able to select the correct port. The port name can be defined as:  kubernetes:v1/pod/test/my-pod?port-name=my-port Debugging the communication between client and the K8s API By default, the okhttp interceptor logging is disabled to avoid polluting the logs with information that might be not needed. However it can be enabled just by setting the following Quarkus property: quarkus.log.category."okhttp3.OkHttpClient".level=INFO ACTION TIME! Let’s see the service discovery engine into action. For this example we have a very simple application that will consume a serverless application running on Minikube with Knative capability enabled. In this you can find information about how to install Minukube and how to enable the Knative addon. With Minikube running with Knative configured, let’s deploy the , for that, execute the following commands: # Create a new namespace and set it as the default $ kubectl create namespace greeting-quarkus $ kubectl config set-context --current --namespace=greeting-quarkus &gt; Note that there is a tool called kubectx that helps to select the default &gt; namespace. To be able to directly build the  container application using the in-cluster Docker Daemon from Minikube, execute the following command: $ eval $(minikube -p minikube docker-env --profile knative) If you haven’t cloned the Kogito Examples repository yet, the next command can help you with this: $ git clone https://github.com/kiegroup/kogito-examples.git Access the serverless-workflow-greeting-quarkus: $ cd kogito-examples/serverless-workflow-examples/serverless-workflow-greeting-quarkus In order to instruct Quarkus to generated and deploy the needed resources automatically, the following extensions needs to be added to the example’s dependencies: &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;kogito-addons-quarkus-knative-eventing&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-kubernetes&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-container-image-jib&lt;/artifactId&gt; &lt;/dependency&gt; After the dependencies were added, build the application with the following command: $ mvn clean package \ -Dquarkus.kubernetes.deployment-target=knative \ -Dquarkus.knative.name=greeting-quarkus-service \ -Dquarkus.container-image.build=true \ -Dquarkus.container-image.registry=dev.local \ -Dquarkus.container-image.group=example \ -Dquarkus.container-image.name=greeting-quarkus-service \ -Dquarkus.container-image.tag=1.0 \ -Dquarkus.kubernetes.deploy=true \ -Dquarkus.container-image.push=false This command will build the application, create a container with the application and deploy in the Minikube using the namespace set before. List the available Knative services: $ kubectl get services.serving.knative.dev greeting-quarkus-service Full result redacted. It should return a URL similar to: http://greeting-quarkus-service.greeting-quarkus.10.104.186.138.sslip.io greeting-quarkus-service-00001 Ok, we can now try to consume the Serverless function: $ curl -X POST -H 'Content-Type:application/json' -H 'Accept:application/json' -d '{"workflowdata" : {"name": "John", "language": "English"}}' http://greeting-quarkus-service.greeting-quarkus.10.104.186.138.sslip.io/jsongreet {"id":"5294a215-e5f4-45ab-b6e1-d3360575b852","workflowdata":{"name":"John","language":"English","greeting":"Hello from JSON Workflow, "}}% Let’s proceed by creating a small application that will consume this Knative Service starting by creating a new application using Quarkus CLI: $ quarkus create app org.kie.kogito:greeting-quarkus-consumer:1.0 --extension=resteasy --extension=org.kie.kogito:kogito-quarkus-serverless-workflow:2.0.0-SNAPSHOT $ cd greeting-quarkus-consumer Import the project in your favorite IDE then, to make things easier, start the Quarkus application we just created in dev mode: $ quarkus dev &gt; TIP: Quarkus Dev service is enabled by default, for this tutorial it is not &gt; needed and can be disable with the following Quarkus property: &gt; quarkus.devservices.enabled=false Let’s prepare our application by adding a few dependencies that will be used to consume the Greeting Quarkus Knative service. First, replace the GreetingResource.java class with the following content: Second, build the URI that will be used to query the Knative Service. For the protocol we can use knative and the GVK will be serving.knative.dev/v1/service, the namespace previously created greeting-quarkus and the resource name which is greeting-quarkus-service, that will result in the following URI: knative:serving.knative.dev/v1/service/greeting-quarkus/greeting-quarkus-service Now add the following system property to the application.properties: my.knative.service=knative:serving.knative.dev/v1/service/greeting-quarkus/greeting-quarkus-service With the dev mode in execution, just call the /hello endpoint: $ curl {“id”:”8d2fcb94-d695-4ebc-8919-66c25bba6765″,”workflowdata”:{“name”:”John”,”language”:”Spanish”,”greeting”:”Saludos desde JSON Workflow, “}} And in the logs you should see something like: Calling url -- http://greeting-quarkus-service.greeting-quarkus.10.104.186.138.sslip.io/jsongreet with payload -&gt; {"workflowdata" : {"name": "John", "language": "Spanish"}} Enable the DEBUG log level to take a closer look at what happened. Add the following system property to the application.properties: quarkus.log.category.”org.kie.kogito.addons.quarkus.k8s”.level=DEBUG And call the /hello endpoint again, you will see some messages like: And call the /hello endpoint again, you will see some messages like: 2022-09-14 16:26:23,422 DEBUG [org.kie.kog.add.qua.k8s.par.KubeURI] (Quarkus Main Thread) KubeURI successfully parsed: KubeURI{protocol='knative', gvk=GVK{group='serving.knative.dev', version='v1', kind='service'}, namespace='greeting-quarkus', resourceName='greeting-quarkus-service', rawUrl='knative:serving.knative.dev/v1/service/greeting-quarkus/greeting-quarkus-service', customPortName='null', customLabels=null} 2022-09-14 16:26:23,518 INFO [org.kie.kog.add.qua.k8s.KubeResourceDiscovery] (Quarkus Main Thread) Connected to kubernetes cluster v1.23.4, current namespace is greeting-quarkus. Resource name for discovery is greeting-quarkus-service 2022-09-14 16:26:23,521 DEBUG [org.kie.kog.add.qua.k8s.KnativeResourceDiscovery] (Quarkus Main Thread) Trying to adapt kubernetes client to knative 2022-09-14 16:26:23,531 DEBUG [org.kie.kog.add.qua.k8s.KnativeResourceDiscovery] (Quarkus Main Thread) Found Knative endpoint at http://greeting-quarkus-service.greeting-quarkus.10.104.186.138.sslip.io As you can see, the URI we have defined was translated by the real endpoint which is bound to the service we want to access. DRAWBACKS As the service discovery feature reads the Quarkus configuration during startup in search of the URI pattern we’ve discussed previously, it can bring a very small delay during the startup, so before moving your application to production you need to decide if you can afford having your application startup delayed by a few milliseconds or seconds. Service Discovery Enabled (main) sw-service-discovery 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.12.2.Final) started in 2.360s. Listening on: http://0.0.0.0:8080 Service Discovery Disabled (main) sw-service-discovery 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.12.2.Final) started in 1.507s. Listening on: http://0.0.0.0:8080 As we can see, the difference is not that big, but it can vary depending on a few things like network latency between the application and the Kubernetes cluster and how many configurations are used.Keep in mind that when using the kogito-quarkus-serverless-workflow extension this feature can be disabled by excluding the kogito-addons-quarkus-kubernetes, as shown below: &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;kogito-quarkus-serverless-workflow&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;kogito-addons-quarkus-kubernetes&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;kogito-addons-quarkus-kubernetes-deployment&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; CONCLUSION The service discovery feature has proven to be very useful since it can discover the most common Kubernetes resources that help expose the application internally in the cluster or to the world. Another advantage of using the service discovery is that, depending if the service has been migrated to another cluster or region that can lead the domain to be changed, the end user does not have to worry about it. The engine will take care of the service endpoint configuration when such migrations or any other updates that can lead to a change to the service happens. The post appeared first on .</content><dc:creator>Filippe Spolti</dc:creator></entry><entry><title>RHEL 9 and single node OpenShift as VMs on macOS Ventura</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/21/rhel-9-and-single-node-openshift-vms-macos-ventura" /><author><name>Michael Hrivnak, Benjamin Schmaus</name></author><id>50766fa8-938d-4d7b-aa7c-bf5772221946</id><updated>2022-10-21T07:00:00Z</updated><published>2022-10-21T07:00:00Z</published><summary type="html">&lt;p&gt;Apple's &lt;a href="https://developer.apple.com/documentation/virtualization"&gt;Virtualization Framework&lt;/a&gt; is gaining new features in the upcoming release of &lt;a href="https://www.apple.com/macos/macos-ventura-preview/"&gt;macOS Ventura&lt;/a&gt; that will make it easy to run ARM &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; virtual machines natively on the M1 processor or other Apple silicon chips. New features of the framework include EFI bootloader support and the ability to render the desktop GUI in a window.&lt;/p&gt; &lt;p&gt;Red Hat recently took a closer look at a beta release of Ventura to see how easy it would be to run virtualized versions of two key technologies—&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; 9 and the single node version of the Red Hat OpenShift Container Platform—on Apple silicon. Spoiler alert: it worked great! Read on for the details and two video demos.&lt;/p&gt; &lt;h2&gt;Running RHEL 9 on Apple silicon&lt;/h2&gt; &lt;p&gt;Developers who use MacBooks or other Apple hardware often find value in utilizing a separate Linux host in order to build, test, and/or run software that is ultimately intended to be deployed on Linux. Running Linux in a local virtual machine is a convenient option that has mostly been a seamless experience on Macs running Intel-based processors. However, users of the latest Apple silicon hardware had hurdles to overcome in order to run ARM-based Linux distributions within local VMs, which often meant that connecting to external Linux infrastructure was the easiest option. But with the virtualization improvements in Ventura, developers using Apple silicon can run a local Linux virtual machine natively on their workstations with a seamless GUI experience and no dependence on connectivity to separate infrastructure.&lt;/p&gt; &lt;p&gt;There can also be advantages to running production Linux-based workloads on Apple silicon. The improvements in Ventura enable Linux distributions with ARM builds, such as RHEL 9, to run on Apple silicon with near-native performance.&lt;/p&gt; &lt;p&gt;Red Hat was able to install RHEL 9 as a guest virtual machine on an M1 Mac running a beta version of Ventura. This video demonstrates the whole process step-by-step.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt;The Virtualization Framework is essentially a software library, so in order to take advantage of the new features, an application for managing the virtual machines was needed that had already integrated the new version of the framework. &lt;a href="https://github.com/utmapp/UTM"&gt;UTM&lt;/a&gt; is an easy-to-use open source application for running virtual machines on macOS, and it had already integrated the new Virtualization Framework into its development branch. A release candidate of UTM 4.0 was used for this demonstration.&lt;/p&gt; &lt;p&gt;Please check out the video above for a preview of what Ventura has to offer. The demo shows how to create a new virtual machine with UTM, walks through the normal RHEL installation wizard, and then explores the newly-created Linux environment.&lt;/p&gt; &lt;p&gt;The demo can be reproduced today using pre-releases of Ventura and UTM. macOS Ventura is expected to be released on October 24, and UTM version 4 is likely to follow soon, having released RC2 on October 10.&lt;/p&gt; &lt;h2&gt;Running single node OpenShift on Apple silicon&lt;/h2&gt; &lt;p&gt;Red Hat has also been able to build on the ability to install RHEL in VMs on Apple silicon to create a demonstration of single node OpenShift running on a Mac. The video below demonstrates the installation of ARM-based single node OpenShift as a virtual machine. The host machine has an M1 processor and a beta release of macOS Ventura.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt;While &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;OpenShift Local&lt;/a&gt; is often the best choice for pure software development, many developers find it useful to run a single node OpenShift VM on their workstations for &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; integration work. Single node OpenShift has also proven its value as an &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt; infrastructure platform.&lt;/p&gt; &lt;p&gt;Apple silicon requires RHEL 9 for ARM to run as a virtual machine. Because OpenShift (as of the current 4.11 release) is still based on top of RHEL 8, the first step was to create an experimental custom build of OpenShift that uses RHEL 9 as its base OS. Since that custom build is not reproducible outside Red Hat, the video above is the only way to see single node OpenShift running on an M1.&lt;/p&gt; &lt;p&gt;But it gives you a preview of what's to come: once OpenShift transitions to a RHEL 9 base operating system, it will be possible to run OpenShift as a virtual machine natively on Apple silicon. And that's big news for the many developers who use Macs as their development machines of choice.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/21/rhel-9-and-single-node-openshift-vms-macos-ventura" title="RHEL 9 and single node OpenShift as VMs on macOS Ventura"&gt;RHEL 9 and single node OpenShift as VMs on macOS Ventura&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Hrivnak, Benjamin Schmaus</dc:creator><dc:date>2022-10-21T07:00:00Z</dc:date></entry><entry><title>The ultimate CI/CD resource guide</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/10/20/ultimate-cicd-resource-guide" /><author><name>Heiker Medina</name></author><id>32737cb1-c7f6-483b-93bb-6729cb90017c</id><updated>2022-10-20T07:00:00Z</updated><published>2022-10-20T07:00:00Z</published><summary type="html">&lt;p&gt;You've found the right place for content on &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration/continuous deployment (CI/CD)&lt;/a&gt;. We've gathered our highest-performing articles from the past year on this topic on Red Hat Developer. This article will introduce you to all things related to CI/CD.&lt;/p&gt; &lt;h2&gt;What is CI/CD and why is it important for developers?&lt;/h2&gt; &lt;p&gt;Continuous integration (CI) and continuous deployment (CD) are development processes making use of automated tools to produce high-quality software.&lt;/p&gt; &lt;p&gt;CI ensures that any code submitted by each developer works together with all other code in the project. Typically, CI works by running regression tests.&lt;/p&gt; &lt;p&gt;CD involves further automation to make sure that the latest accepted versions of a project enter production, and that all the pieces deployed together are compatible.&lt;/p&gt; &lt;p&gt;Numerous tools, such as integrated development environments and version control systems, help you build software. But when it comes to creating software that customers trust—and even love—you need to pay attention to the details. A good CI/CD environment ensures that testing, integration, and deployment are fast, easy, and accurate. CI/CD allows you to iterate faster, build more reliable code, and deliver better customer experiences.&lt;/p&gt; &lt;h2&gt;Recent articles to explore about CI/CD&lt;/h2&gt; &lt;p&gt;Here are eight great Red Hat Developer articles on this topic:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/24/deploy-helm-charts-jenkins-cicd-red-hat-openshift-4"&gt;Deploy Helm charts with Jenkins CI/CD&lt;/a&gt;:&lt;/strong&gt; Learn how to use Jenkins's CI/CD capabilities to deploy a Helm chart using a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; 4 cluster. Helm is a package manager for Kubernetes that uses a packaging format called &lt;em&gt;charts&lt;/em&gt;. These charts have all of the Kubernetes resources required to deploy an application, such as deployments and services.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/13/developers-guide-cicd-and-gitops-jenkins-pipelines"&gt;A developer's guide to CI/CD and GitOps with Jenkins Pipelines&lt;/a&gt;:&lt;/strong&gt; &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; and CI/CD have a lot to offer each other. This article guides you through the use of a Jenkinsfile to create deployments that combine CI/CD and GitOps.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/08/01/containerize-net-applications-without-writing-dockerfiles"&gt;Containerize .NET applications without writing Dockerfiles&lt;/a&gt;:&lt;/strong&gt; Discover how to use a tool named dotnet build-image to create Dockerfiles and containerized images from &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET&lt;/a&gt; applications. You will also learn how to use this tool in a GitHub workflow to create an image from a .NET application and push it to a repository.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/05/automate-cicd-pull-requests-argo-cd-applicationsets#"&gt;Automate CI/CD on pull requests with Argo CD ApplicationSets&lt;/a&gt;:&lt;/strong&gt; The quest to further automate building, testing, and deployment is inspiring new features in Argo CD, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, and other tools. This article shows how to improve feature testing by automating builds and the creation of Kubernetes environments. ApplicationSets in Argo CD, together with Tekton, create a CI/CD system that includes feature branch testing on OpenShift.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/01/how-create-better-front-end-developer-experience"&gt;How to create a better front-end developer experience&lt;/a&gt;:&lt;/strong&gt; Explore common pain points that can complicate the development process, and learn how to address them to foster better developer experiences. Take developing a form using React, for example. If developers can develop the form without difficulty, it will likely be a positive experience for the customer as well.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/09/gitops-using-red-hat-openshift-console-49-and-410#"&gt;OpenShift support for GitOps processes&lt;/a&gt;:&lt;/strong&gt; This article explains how &lt;a href="https://next.redhat.com/project/gitops-primer/"&gt;OpenShift's GitOps Operator&lt;/a&gt; works and highlights improvements in OpenShift 4.9 and 4.10. The article includes a video showing how to use the developer console to manage cluster configurations and deploy cloud-native applications using OpenShift. Follow up this article by trying out a &lt;a href="https://developers.redhat.com/learn/openshift/develop-gitops"&gt;learning path about GitOps&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/20/integrate-iso-20022-payments-messaging-cicd"&gt;Integrate ISO 20022 payments messaging with CI/CD&lt;/a&gt;:&lt;/strong&gt; An increasing number of financial institutions are embracing ISO 20022 standards for payments and securities transactions. This article shows how to use the message conversion platform from Trace Financial (a Red Hat ISV) and Red Hat Fuse to convert messages between the SWIFT MT and MX formats for financial data. Key benefits of the MX message set include the ability to capture richer data, flexibility, and a machine-readable format.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/12/tools-and-practices-remote-development-teams"&gt;Tools and practices for remote development teams&lt;/a&gt;:&lt;/strong&gt; This article explores a few tools and practices that can help you work from home. Ideas include shared IDEs to facilitate collaboration, a repository of self-service and single-click workspaces for IDE consistency, and triggering CI/CD pipelines from source control systems to automate manual operations tasks.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The Developer Sandbox is a great place to start&lt;/h2&gt; &lt;p&gt;Interested in developing your first application at no cost? Test out the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox on Red Hat OpenShift&lt;/a&gt; and learn by doing.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/10/20/ultimate-cicd-resource-guide" title="The ultimate CI/CD resource guide"&gt;The ultimate CI/CD resource guide&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Heiker Medina</dc:creator><dc:date>2022-10-20T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 20 October 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-10-20.html" /><category term="kogito" /><category term="drools" /><category term="quarkus" /><category term="resteasy" /><category term="byteman" /><category term="wildfly" /><author><name>Jason Porter</name><uri>https://www.jboss.org/people/jason-porter</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-10-20.html</id><updated>2022-10-20T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="kogito, drools, quarkus, resteasy, byteman, wildfly"&gt; &lt;h1&gt;This Week in JBoss - 20 October 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Welcome back everyone! We’re here with another JBoss Editorial, highlighting some of the great work done in the community and within Red Hat as it relates to middleware. Like always, we have some news from the blogosphere and releases. No new videos out this week. Let’s get started!&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases"&gt;Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/10/kogito-1-29-0-released.html"&gt;Kogito 1.29.0.Final&lt;/a&gt; - Mostly a bug fix release, but there are some breaking changes, please review before upgrading&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/10/drools-8-final-toward-a-modular-and-cloud-native-rule-engine.html"&gt;Drools 8&lt;/a&gt; - A brand new major release for Drools! This is great news for everyone. New features abound, and migrations as well. Be sure to check it out and read about everything that is new.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-13-2-final-released/"&gt;Quarkus 2.13.2.Final&lt;/a&gt; - Bug fixes and documentation improvements abound in this release. If you’re on 2.13, be sure to upgrade!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.dev/2022/10/10/resteasy-6.2.1.Final-release/"&gt;RESTEasy 6.2.1.Final&lt;/a&gt; - Some bug fixes, mostly component upgrades&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://bytemanblog.blogspot.com/2022/10/byteman-4020-has-been-released.html"&gt;Byteman 4.0.20&lt;/a&gt; - This release works all the way up to JDK 20, and also includes a couple of bug fixes!&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_blogs"&gt;Blogs&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/eclipse/eclipse-microservices/how-to-run-artemis-messaging-in-a-bootable-jar/"&gt;How to run Artemis Messaging in a Bootable Jar&lt;/a&gt; - If you’re running a Wildfly application as a bootable jar, you’ll want to take note of this. Running your application as a bootable jar is a great way to get started moving towards containers, or even building a container around the bootable jar.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-newsletter-25/"&gt;Quarkus Newsletter 25&lt;/a&gt; - If you’re looking to catch up with what is happening in Quarkus, look no further. James Cobb gives us a great rundown of things happening in the Quarkus community.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/hacktoberfest/"&gt;Hacktoberfest&lt;/a&gt; - There’s still time to get involved in Hacktoberfest! Be sure to check it out and join others in building better Open Source Software.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;Thanks everyone for visiting, we hope to see you back next time!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/jason-porter.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Jason Porter&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Jason Porter</dc:creator></entry></feed>
